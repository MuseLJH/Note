# 网络爬虫

## 爬虫简介

### 基本流程

- 发起请求

  通过HTTP库想目标站点发起请求，即发送一个Request，请求可以包含额外的headers等信息，等待服务器响应。

+ 获取响应内容

  如果服务器能正常响应，会得到一个Response，Response的内容边是所要获取的页面内容，类型可能有HTML、Json字符串、二进制数据（如图片视频）等类型。

+ 解析内容

  得到的内容可能是HTML，可以用正则表达式、网页解析库进行解析。可能是Json，可以直接转为Json对象解析。可能是二进制数据，可以做保存或者进一步的处理。

+ 保存数据

  保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。
## 环境配置

### Py3的安装

+ Anaconda安装
+ Py3安装

### 请求库的安装

#### Requests
#### Selenium
#### ChromeDriver
#### GeckoDriver

在上一节我们了解了 ChromeDriver 的配置方法，配置完成之后我们便可以用 Selenium 来驱动 Chrome 浏览器来做相应网页的抓取。

那么对于 Firefox 来说，也可以使用同样的方式完成 Selenium 的对接，这时需要安装另一个驱动 GeckoDriver。

```shell
wget https://github.com/mozilla/geckodriver/releases/download/v0.18.0/geckodriver-v0.18.0-linux64.tar.gz
tar -xvzf geckodriver*
chmod +x geckodriver
export PATH=$PATH:/path-to-extracted-file/geckodriver
```

#### PhantomJS

如果我们使用 Chrome 或 Firefox 进行网页抓取的话，每次抓取的时候，都会弹出一个浏览器，比较影响使用。所以在这里再介绍一个无界面浏览器，叫做 PhantomJS。

PhantomJS 是一个无界面的，可脚本编程的 WebKit 浏览器引擎。它原生支持多种 web 标准：DOM 操作，CSS 选择器，JSON，Canvas 以及 SVG。

Selenium 支持 PhantomJS，这样在运行的时候就不会再弹出一个浏览器了，而且其运行效率也是很高的，还支持各种参数配置，使用非常方便，下面我们就来了解一下 PhantomJS 的安装过程。

##### 1.相关链接

- 官方网站：[http://phantomjs.org](http://phantomjs.org/)
- 官方文档：<http://phantomjs.org/quick-start.html>
- 下载地址：<http://phantomjs.org/download.html>
- API接口说明：<http://phantomjs.org/api/command-line.html>

##### 2. 下载 PhantomJS

我们需要在官方网站下载对应的安装包。<http://phantomjs.org/download.html>，它支持多种操作系统，Windows、Linux、Mac、FreeBSD 等，我们可以选择对应的平台将安装包下载下来。

```shell
sudo apt-get update
sudo apt-get install build-essential chrpath libssl-dev libxft-dev -y
sudo apt-get install libfreetype6 libfreetype6-dev -y
sudo apt-get install libfontconfig1 libfontconfig1-dev -y
export PHANTOM_JS="phantomjs-2.1.1-linux-x86_64"
wget https://github.com/Medium/phantomjs/releases/download/v2.1.1/$PHANTOM_JS.tar.bz2
sudo tar xvjf $PHANTOM_JS.tar.bz2
sudo mv $PHANTOM_JS /usr/local/share
sudo ln -sf /usr/local/share/$PHANTOM_JS/bin/phantomjs /usr/local/bin
phantomjs --version

# 上面那种下载可能比较慢，可先去官网下载文件，然后再添加环境变量
export PHANTOM_JS="phantomjs-2.1.1-linux-x86_64"
sudo tar xvjf $PHANTOM_JS.tar.bz2
# 剩下就是一样的
```



#### Aiohttp

### 解析库的安装

#### LXML
#### BeautifulSoup
#### PyQuery
#### Tesserocr

### 数据库的安装

#### MySQL
#### MongoDB
#### Redis

### 存储库的安装

#### PyMySQL
#### PyMongo
#### RedisPy
#### RedisDump

### Web库的安装

#### Flask
#### Tornado

### APP爬取相关库的安装

#### Charles
#### MitmProxy
#### Appium

### 爬虫框架的安装

#### PySpider
#### Scrapy
#### ScrapySplash

ScrapySplash 是一个 Scrapy 中支持 JavaScript 渲染的工具，本节来介绍一下它的安装方式。ScrapySplash 的安装分为两部分，一个是是 Splash 服务的安装，安装方式是通过 Docker，安装之后会启动一个 Splash 服务，我们可以通过它的接口来实现 JavaScript 页面的加载。另外一个是 ScrapySplash 的 Python 库的安装，安装之后即可在 Scrapy 中使用 Splash 服务。

+ 安装Splash

  ```shell
  sudo docker run -p 8050:8050 scrapinghub/splash
  ```

+ 安装信息

  ```shell
  # 安装信息：
  Unable to find image 'scrapinghub/splash:latest' locally
  latest: Pulling from scrapinghub/splash
  1be7f2b886e8: Pull complete 
  6fbc4a21b806: Pull complete 
  c71a6f8e1378: Pull complete 
  4be3072e5a37: Pull complete 
  06c6d2f59700: Pull complete 
  ba70d52f4d83: Pull complete 
  e9f7760ce7af: Pull complete 
  16259f7c35ad: Pull complete 
  efb2fb9620cf: Pull complete 
  31e4491cc70e: Pull complete 
  Digest: sha256:f768f56cc261d1087072ed4ae4c30dc52672d1244ffe179ba43adb6c5e9c430b
  Status: Downloaded newer image for scrapinghub/splash:latest
  2018-10-25 13:19:08+0000 [-] Log opened.
  2018-10-25 13:19:08.431431 [-] Splash version: 3.2
  2018-10-25 13:19:08.431928 [-] Qt 5.9.1, PyQt 5.9, WebKit 602.1, sip 4.19.3, Twisted 16.1.1, Lua 5.2
  2018-10-25 13:19:08.432147 [-] Python 3.5.2 (default, Nov 23 2017, 16:37:01) [GCC 5.4.0 20160609]
  2018-10-25 13:19:08.432263 [-] Open files limit: 1048576
  2018-10-25 13:19:08.432374 [-] Can't bump open files limit
  2018-10-25 13:19:08.535588 [-] Xvfb is started: ['Xvfb', ':329517549', '-screen', '0', '1024x768x24', '-nolisten', 'tcp']
  QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'
  2018-10-25 13:19:08.600898 [-] proxy profiles support is enabled, proxy profiles path: /etc/splash/proxy-profiles
  2018-10-25 13:19:08.686105 [-] verbosity=1
  2018-10-25 13:19:08.686241 [-] slots=50
  2018-10-25 13:19:08.686342 [-] argument_cache_max_entries=500
  2018-10-25 13:19:08.686671 [-] Web UI: enabled, Lua: enabled (sandbox: enabled)
  2018-10-25 13:19:08.686816 [-] Server listening on 0.0.0.0:8050
  2018-10-25 13:19:08.687497 [-] Site starting on 8050
  2018-10-25 13:19:08.687598 [-] Starting factory <twisted.web.server.Site object at 0x7f9a5c6497f0>
  ```

+ 这样就证明 Splash 已经在 8050 端口上运行了。

  这时我们打开：[http://localhost:8050](http://localhost:8050/) 即可看到 Splash 的主页，如图所示：

  ![1540473907680](C:\Users\LegnaVI\AppData\Roaming\Typora\typora-user-images\1540473907680.png)

当然 Splash 也可以直接安装在远程服务器上，我们在服务器上运行以守护态运行 Splash 即可，命令如下：

```shell
docker run -d -p 8050:8050 scrapinghub/splash
```

在这里多了一个 -d 参数，它代表将 Docker 容器以守护态运行，这样在中断远程服务器连接后不会终止 Splash 服务的运行。

成功安装了 Splash 之后，我们接下来再来安装一下其 Python 库，安装命令如下：

```shell
pip3 install scrapy-splash
```

命令运行完毕后就会成功安装好此库，后文我们会介绍它的详细用法。

#### ScrapyRedis

### 部署相关库的安装

#### Docker
#### Scrapyd
#### ScrapyClient
#### ScrapyAPI
#### Scrapyrt
#### Gerapy

## 爬虫基础

### HTTP基本原理

### Web网页基础

### 爬虫基本原理

### Session和Cookies

### 代理基本原理

### 正则表达式

## 基本库的使用

### 使用Urllib

#### 发送请求

####  处理异常

#### 解析链接

#### 分析Robots协议

### 使用Requests

#### 基本使用

#### 高级用法

### 爬取猫眼电影排行

## 解析库的使用

### Xpath的使用

### BeautifulSoup的使用

### PyQuery的使用

## 数据存储

### 文件存储

#### TXT文本存储

#### Json文件存储

#### CSV文件存储

### 关系型数据库存储

####  MySQL存储

### 非关系型数据库存储

####  MongoDB存储

####  Redis存储

## Ajax数据爬取

###  什么是Ajax

### Ajax分析方法

### Ajax结果提取

### 分析Ajax爬取今日头条街拍美图

## 动态渲染页面抓取

### Selenium的使用

### Splash的使用

### Splash负载均衡配置

### 使用Selenium爬取淘宝商品

## 验证码的识别

### 图形验证码

### 极验滑动验证码

### 点触验证码

### 微博宫格验证码

## 代理的使用

### 代理的设置

### 代理池的维护

### 付费代理的使用

### ADSL代理的使用

### 使用代理爬取微信公众号的文章

## 模拟登录

### 模拟登录并爬取GitHub

### Cookies池的搭建

## APP的爬取

### Charles的使用

### MitmProxy的使用

### MitmDump爬取得到APP电子书信息

### Appium的使用

### Appium爬取微信朋友圈

### Appium+MitmProxy爬取京东商品评论

## Pyspider框架的使用

### 框架介绍

### 基本使用

### 用法详解

## Scrapy框架的使用

### Scrapy框架详解

### Scrapy入门

### Selector的用法

### Spider的用法

### Downloader Middleware的用法

### Spider Middleware的用法

### Item Pipeline的用法

### Scrapy对接Selenium

### Scrapy对接Splash
### Scrapy通用爬虫
### Scrapyrt的使用
### Scrapy对接Docker
### Scrapy爬取新浪微博


## 分布式爬虫
### 分布式爬虫理念
### ScrapyRedis源码解析
### Scrapy分布式实现
### BloomFilter的对接
## 分布式爬虫的部署
### Scrapyd分布式部署
### ScrapydClient的使用
### Scrapyd对接的Docker
### Scrapyd批量部署
### Gerapy分布式管理
